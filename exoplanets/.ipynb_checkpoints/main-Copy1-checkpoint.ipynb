{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import svm, tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "# df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>198.78</td>\n",
       "      <td>211.28</td>\n",
       "      <td>187.27</td>\n",
       "      <td>202.34</td>\n",
       "      <td>211.43</td>\n",
       "      <td>208.74</td>\n",
       "      <td>230.06</td>\n",
       "      <td>175.02</td>\n",
       "      <td>175.70</td>\n",
       "      <td>...</td>\n",
       "      <td>131.66</td>\n",
       "      <td>143.17</td>\n",
       "      <td>190.08</td>\n",
       "      <td>138.63</td>\n",
       "      <td>148.90</td>\n",
       "      <td>151.77</td>\n",
       "      <td>83.11</td>\n",
       "      <td>84.24</td>\n",
       "      <td>105.44</td>\n",
       "      <td>117.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.53</td>\n",
       "      <td>11.54</td>\n",
       "      <td>14.44</td>\n",
       "      <td>13.30</td>\n",
       "      <td>11.29</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.00</td>\n",
       "      <td>8.70</td>\n",
       "      <td>11.14</td>\n",
       "      <td>...</td>\n",
       "      <td>7.54</td>\n",
       "      <td>7.43</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>14.06</td>\n",
       "      <td>3.41</td>\n",
       "      <td>12.41</td>\n",
       "      <td>-12.72</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-9.64</td>\n",
       "      <td>-8.82</td>\n",
       "      <td>-11.90</td>\n",
       "      <td>-9.24</td>\n",
       "      <td>-7.26</td>\n",
       "      <td>-6.44</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>18.89</td>\n",
       "      <td>18.15</td>\n",
       "      <td>20.51</td>\n",
       "      <td>13.78</td>\n",
       "      <td>20.28</td>\n",
       "      <td>20.91</td>\n",
       "      <td>-7.91</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-18.31</td>\n",
       "      <td>4.13</td>\n",
       "      <td>76.75</td>\n",
       "      <td>9.38</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>5.44</td>\n",
       "      <td>-63.56</td>\n",
       "      <td>27.00</td>\n",
       "      <td>-65.00</td>\n",
       "      <td>...</td>\n",
       "      <td>57.00</td>\n",
       "      <td>41.69</td>\n",
       "      <td>75.75</td>\n",
       "      <td>99.63</td>\n",
       "      <td>59.88</td>\n",
       "      <td>71.57</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>86.07</td>\n",
       "      <td>171.57</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>245.06</td>\n",
       "      <td>177.37</td>\n",
       "      <td>174.62</td>\n",
       "      <td>47.12</td>\n",
       "      <td>33.50</td>\n",
       "      <td>-103.66</td>\n",
       "      <td>-127.13</td>\n",
       "      <td>-153.78</td>\n",
       "      <td>-211.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1015.00</td>\n",
       "      <td>1054.44</td>\n",
       "      <td>975.03</td>\n",
       "      <td>1009.53</td>\n",
       "      <td>1016.22</td>\n",
       "      <td>1065.31</td>\n",
       "      <td>1014.09</td>\n",
       "      <td>1022.56</td>\n",
       "      <td>1043.37</td>\n",
       "      <td>1075.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>1</td>\n",
       "      <td>6.45</td>\n",
       "      <td>3.95</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>5.16</td>\n",
       "      <td>14.89</td>\n",
       "      <td>19.36</td>\n",
       "      <td>6.13</td>\n",
       "      <td>19.24</td>\n",
       "      <td>3.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>37.19</td>\n",
       "      <td>35.75</td>\n",
       "      <td>63.11</td>\n",
       "      <td>30.55</td>\n",
       "      <td>42.53</td>\n",
       "      <td>63.00</td>\n",
       "      <td>90.28</td>\n",
       "      <td>64.55</td>\n",
       "      <td>74.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>1</td>\n",
       "      <td>210.50</td>\n",
       "      <td>162.72</td>\n",
       "      <td>148.73</td>\n",
       "      <td>144.62</td>\n",
       "      <td>150.96</td>\n",
       "      <td>70.64</td>\n",
       "      <td>55.06</td>\n",
       "      <td>70.12</td>\n",
       "      <td>67.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.68</td>\n",
       "      <td>-78.74</td>\n",
       "      <td>-142.32</td>\n",
       "      <td>-176.99</td>\n",
       "      <td>-224.43</td>\n",
       "      <td>-282.02</td>\n",
       "      <td>-315.21</td>\n",
       "      <td>-308.97</td>\n",
       "      <td>-238.11</td>\n",
       "      <td>-188.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>1</td>\n",
       "      <td>1373.12</td>\n",
       "      <td>1460.25</td>\n",
       "      <td>1543.87</td>\n",
       "      <td>1585.65</td>\n",
       "      <td>1689.44</td>\n",
       "      <td>1775.72</td>\n",
       "      <td>1845.50</td>\n",
       "      <td>1952.56</td>\n",
       "      <td>2052.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-656.03</td>\n",
       "      <td>-607.41</td>\n",
       "      <td>-476.91</td>\n",
       "      <td>-375.69</td>\n",
       "      <td>-274.31</td>\n",
       "      <td>-99.66</td>\n",
       "      <td>-2032.16</td>\n",
       "      <td>-1924.44</td>\n",
       "      <td>-1874.25</td>\n",
       "      <td>-1796.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>1</td>\n",
       "      <td>30.01</td>\n",
       "      <td>42.47</td>\n",
       "      <td>25.59</td>\n",
       "      <td>27.33</td>\n",
       "      <td>38.41</td>\n",
       "      <td>26.70</td>\n",
       "      <td>24.14</td>\n",
       "      <td>16.66</td>\n",
       "      <td>23.63</td>\n",
       "      <td>...</td>\n",
       "      <td>24.51</td>\n",
       "      <td>34.49</td>\n",
       "      <td>24.45</td>\n",
       "      <td>14.93</td>\n",
       "      <td>10.78</td>\n",
       "      <td>25.11</td>\n",
       "      <td>9.15</td>\n",
       "      <td>29.58</td>\n",
       "      <td>27.07</td>\n",
       "      <td>19.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>1</td>\n",
       "      <td>-591.74</td>\n",
       "      <td>-571.56</td>\n",
       "      <td>-580.32</td>\n",
       "      <td>-522.65</td>\n",
       "      <td>-496.57</td>\n",
       "      <td>-293.84</td>\n",
       "      <td>-95.74</td>\n",
       "      <td>-430.12</td>\n",
       "      <td>-466.17</td>\n",
       "      <td>...</td>\n",
       "      <td>60.95</td>\n",
       "      <td>21.38</td>\n",
       "      <td>28.43</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-20.14</td>\n",
       "      <td>-50.59</td>\n",
       "      <td>-69.67</td>\n",
       "      <td>-53.18</td>\n",
       "      <td>-69.59</td>\n",
       "      <td>-53.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4808 rows × 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6   FLUX.7  \\\n",
       "0         1   198.78   211.28   187.27   202.34   211.43   208.74   230.06   \n",
       "1         1    16.53    11.54    14.44    13.30    11.29     5.35    14.00   \n",
       "2         1    -9.64    -8.82   -11.90    -9.24    -7.26    -6.44    -2.58   \n",
       "3         1   -18.31     4.13    76.75     9.38    -9.81     5.44   -63.56   \n",
       "4         1   245.06   177.37   174.62    47.12    33.50  -103.66  -127.13   \n",
       "...     ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "4803      1     6.45     3.95    -3.72     5.16    14.89    19.36     6.13   \n",
       "4804      1   210.50   162.72   148.73   144.62   150.96    70.64    55.06   \n",
       "4805      1  1373.12  1460.25  1543.87  1585.65  1689.44  1775.72  1845.50   \n",
       "4806      1    30.01    42.47    25.59    27.33    38.41    26.70    24.14   \n",
       "4807      1  -591.74  -571.56  -580.32  -522.65  -496.57  -293.84   -95.74   \n",
       "\n",
       "       FLUX.8   FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0      175.02   175.70  ...     131.66     143.17     190.08     138.63   \n",
       "1        8.70    11.14  ...       7.54       7.43      -2.09      14.06   \n",
       "2       -2.18    -3.13  ...      18.89      18.15      20.51      13.78   \n",
       "3       27.00   -65.00  ...      57.00      41.69      75.75      99.63   \n",
       "4     -153.78  -211.56  ...    1015.00    1054.44     975.03    1009.53   \n",
       "...       ...      ...  ...        ...        ...        ...        ...   \n",
       "4803    19.24     3.45  ...      -9.28      37.19      35.75      63.11   \n",
       "4804    70.12    67.70  ...     -21.68     -78.74    -142.32    -176.99   \n",
       "4805  1952.56  2052.97  ...    -656.03    -607.41    -476.91    -375.69   \n",
       "4806    16.66    23.63  ...      24.51      34.49      24.45      14.93   \n",
       "4807  -430.12  -466.17  ...      60.95      21.38      28.43      -3.49   \n",
       "\n",
       "      FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0        148.90     151.77      83.11      84.24     105.44     117.55  \n",
       "1          3.41      12.41     -12.72       4.81      -2.87       2.59  \n",
       "2         20.28      20.91      -7.91      -1.59      -2.48      -3.20  \n",
       "3         59.88      71.57      -0.69      86.07     171.57      75.00  \n",
       "4       1016.22    1065.31    1014.09    1022.56    1043.37    1075.69  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "4803      30.55      42.53      63.00      90.28      64.55      74.83  \n",
       "4804    -224.43    -282.02    -315.21    -308.97    -238.11    -188.83  \n",
       "4805    -274.31     -99.66   -2032.16   -1924.44   -1874.25   -1796.19  \n",
       "4806      10.78      25.11       9.15      29.58      27.07      19.05  \n",
       "4807     -20.14     -50.59     -69.67     -53.18     -69.59     -53.81  \n",
       "\n",
       "[4808 rows x 3198 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>198.78</td>\n",
       "      <td>211.28</td>\n",
       "      <td>187.27</td>\n",
       "      <td>202.34</td>\n",
       "      <td>211.43</td>\n",
       "      <td>208.74</td>\n",
       "      <td>230.06</td>\n",
       "      <td>175.02</td>\n",
       "      <td>175.70</td>\n",
       "      <td>...</td>\n",
       "      <td>131.66</td>\n",
       "      <td>143.17</td>\n",
       "      <td>190.08</td>\n",
       "      <td>138.63</td>\n",
       "      <td>148.90</td>\n",
       "      <td>151.77</td>\n",
       "      <td>83.11</td>\n",
       "      <td>84.24</td>\n",
       "      <td>105.44</td>\n",
       "      <td>117.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16.53</td>\n",
       "      <td>11.54</td>\n",
       "      <td>14.44</td>\n",
       "      <td>13.30</td>\n",
       "      <td>11.29</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.00</td>\n",
       "      <td>8.70</td>\n",
       "      <td>11.14</td>\n",
       "      <td>...</td>\n",
       "      <td>7.54</td>\n",
       "      <td>7.43</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>14.06</td>\n",
       "      <td>3.41</td>\n",
       "      <td>12.41</td>\n",
       "      <td>-12.72</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.64</td>\n",
       "      <td>-8.82</td>\n",
       "      <td>-11.90</td>\n",
       "      <td>-9.24</td>\n",
       "      <td>-7.26</td>\n",
       "      <td>-6.44</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>18.89</td>\n",
       "      <td>18.15</td>\n",
       "      <td>20.51</td>\n",
       "      <td>13.78</td>\n",
       "      <td>20.28</td>\n",
       "      <td>20.91</td>\n",
       "      <td>-7.91</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-18.31</td>\n",
       "      <td>4.13</td>\n",
       "      <td>76.75</td>\n",
       "      <td>9.38</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>5.44</td>\n",
       "      <td>-63.56</td>\n",
       "      <td>27.00</td>\n",
       "      <td>-65.00</td>\n",
       "      <td>...</td>\n",
       "      <td>57.00</td>\n",
       "      <td>41.69</td>\n",
       "      <td>75.75</td>\n",
       "      <td>99.63</td>\n",
       "      <td>59.88</td>\n",
       "      <td>71.57</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>86.07</td>\n",
       "      <td>171.57</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>245.06</td>\n",
       "      <td>177.37</td>\n",
       "      <td>174.62</td>\n",
       "      <td>47.12</td>\n",
       "      <td>33.50</td>\n",
       "      <td>-103.66</td>\n",
       "      <td>-127.13</td>\n",
       "      <td>-153.78</td>\n",
       "      <td>-211.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1015.00</td>\n",
       "      <td>1054.44</td>\n",
       "      <td>975.03</td>\n",
       "      <td>1009.53</td>\n",
       "      <td>1016.22</td>\n",
       "      <td>1065.31</td>\n",
       "      <td>1014.09</td>\n",
       "      <td>1022.56</td>\n",
       "      <td>1043.37</td>\n",
       "      <td>1075.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>0</td>\n",
       "      <td>6.45</td>\n",
       "      <td>3.95</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>5.16</td>\n",
       "      <td>14.89</td>\n",
       "      <td>19.36</td>\n",
       "      <td>6.13</td>\n",
       "      <td>19.24</td>\n",
       "      <td>3.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>37.19</td>\n",
       "      <td>35.75</td>\n",
       "      <td>63.11</td>\n",
       "      <td>30.55</td>\n",
       "      <td>42.53</td>\n",
       "      <td>63.00</td>\n",
       "      <td>90.28</td>\n",
       "      <td>64.55</td>\n",
       "      <td>74.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>0</td>\n",
       "      <td>210.50</td>\n",
       "      <td>162.72</td>\n",
       "      <td>148.73</td>\n",
       "      <td>144.62</td>\n",
       "      <td>150.96</td>\n",
       "      <td>70.64</td>\n",
       "      <td>55.06</td>\n",
       "      <td>70.12</td>\n",
       "      <td>67.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.68</td>\n",
       "      <td>-78.74</td>\n",
       "      <td>-142.32</td>\n",
       "      <td>-176.99</td>\n",
       "      <td>-224.43</td>\n",
       "      <td>-282.02</td>\n",
       "      <td>-315.21</td>\n",
       "      <td>-308.97</td>\n",
       "      <td>-238.11</td>\n",
       "      <td>-188.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>0</td>\n",
       "      <td>1373.12</td>\n",
       "      <td>1460.25</td>\n",
       "      <td>1543.87</td>\n",
       "      <td>1585.65</td>\n",
       "      <td>1689.44</td>\n",
       "      <td>1775.72</td>\n",
       "      <td>1845.50</td>\n",
       "      <td>1952.56</td>\n",
       "      <td>2052.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-656.03</td>\n",
       "      <td>-607.41</td>\n",
       "      <td>-476.91</td>\n",
       "      <td>-375.69</td>\n",
       "      <td>-274.31</td>\n",
       "      <td>-99.66</td>\n",
       "      <td>-2032.16</td>\n",
       "      <td>-1924.44</td>\n",
       "      <td>-1874.25</td>\n",
       "      <td>-1796.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>0</td>\n",
       "      <td>30.01</td>\n",
       "      <td>42.47</td>\n",
       "      <td>25.59</td>\n",
       "      <td>27.33</td>\n",
       "      <td>38.41</td>\n",
       "      <td>26.70</td>\n",
       "      <td>24.14</td>\n",
       "      <td>16.66</td>\n",
       "      <td>23.63</td>\n",
       "      <td>...</td>\n",
       "      <td>24.51</td>\n",
       "      <td>34.49</td>\n",
       "      <td>24.45</td>\n",
       "      <td>14.93</td>\n",
       "      <td>10.78</td>\n",
       "      <td>25.11</td>\n",
       "      <td>9.15</td>\n",
       "      <td>29.58</td>\n",
       "      <td>27.07</td>\n",
       "      <td>19.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>0</td>\n",
       "      <td>-591.74</td>\n",
       "      <td>-571.56</td>\n",
       "      <td>-580.32</td>\n",
       "      <td>-522.65</td>\n",
       "      <td>-496.57</td>\n",
       "      <td>-293.84</td>\n",
       "      <td>-95.74</td>\n",
       "      <td>-430.12</td>\n",
       "      <td>-466.17</td>\n",
       "      <td>...</td>\n",
       "      <td>60.95</td>\n",
       "      <td>21.38</td>\n",
       "      <td>28.43</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-20.14</td>\n",
       "      <td>-50.59</td>\n",
       "      <td>-69.67</td>\n",
       "      <td>-53.18</td>\n",
       "      <td>-69.59</td>\n",
       "      <td>-53.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4808 rows × 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6   FLUX.7  \\\n",
       "0         0   198.78   211.28   187.27   202.34   211.43   208.74   230.06   \n",
       "1         0    16.53    11.54    14.44    13.30    11.29     5.35    14.00   \n",
       "2         0    -9.64    -8.82   -11.90    -9.24    -7.26    -6.44    -2.58   \n",
       "3         0   -18.31     4.13    76.75     9.38    -9.81     5.44   -63.56   \n",
       "4         0   245.06   177.37   174.62    47.12    33.50  -103.66  -127.13   \n",
       "...     ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "4803      0     6.45     3.95    -3.72     5.16    14.89    19.36     6.13   \n",
       "4804      0   210.50   162.72   148.73   144.62   150.96    70.64    55.06   \n",
       "4805      0  1373.12  1460.25  1543.87  1585.65  1689.44  1775.72  1845.50   \n",
       "4806      0    30.01    42.47    25.59    27.33    38.41    26.70    24.14   \n",
       "4807      0  -591.74  -571.56  -580.32  -522.65  -496.57  -293.84   -95.74   \n",
       "\n",
       "       FLUX.8   FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0      175.02   175.70  ...     131.66     143.17     190.08     138.63   \n",
       "1        8.70    11.14  ...       7.54       7.43      -2.09      14.06   \n",
       "2       -2.18    -3.13  ...      18.89      18.15      20.51      13.78   \n",
       "3       27.00   -65.00  ...      57.00      41.69      75.75      99.63   \n",
       "4     -153.78  -211.56  ...    1015.00    1054.44     975.03    1009.53   \n",
       "...       ...      ...  ...        ...        ...        ...        ...   \n",
       "4803    19.24     3.45  ...      -9.28      37.19      35.75      63.11   \n",
       "4804    70.12    67.70  ...     -21.68     -78.74    -142.32    -176.99   \n",
       "4805  1952.56  2052.97  ...    -656.03    -607.41    -476.91    -375.69   \n",
       "4806    16.66    23.63  ...      24.51      34.49      24.45      14.93   \n",
       "4807  -430.12  -466.17  ...      60.95      21.38      28.43      -3.49   \n",
       "\n",
       "      FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0        148.90     151.77      83.11      84.24     105.44     117.55  \n",
       "1          3.41      12.41     -12.72       4.81      -2.87       2.59  \n",
       "2         20.28      20.91      -7.91      -1.59      -2.48      -3.20  \n",
       "3         59.88      71.57      -0.69      86.07     171.57      75.00  \n",
       "4       1016.22    1065.31    1014.09    1022.56    1043.37    1075.69  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "4803      30.55      42.53      63.00      90.28      64.55      74.83  \n",
       "4804    -224.43    -282.02    -315.21    -308.97    -238.11    -188.83  \n",
       "4805    -274.31     -99.66   -2032.16   -1924.44   -1874.25   -1796.19  \n",
       "4806      10.78      25.11       9.15      29.58      27.07      19.05  \n",
       "4807     -20.14     -50.59     -69.67     -53.18     -69.59     -53.81  \n",
       "\n",
       "[4808 rows x 3198 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['LABEL'] = df_train['LABEL'].apply(lambda x: 0 if x==1 else 1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximo:\t4299288.00\n",
      "Minimo:\t-2385019.12\n",
      "Media:\t126.86\n",
      "Desviacion estandar: 21826.89\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Maximo:\\t{df_train[df_train.columns[1:]].values.max():.2f}\n",
    "Minimo:\\t{df_train[df_train.columns[1:]].values.min():.2f}\n",
    "Media:\\t{df_train[df_train.columns[1:]].values.mean():.2f}\n",
    "Desviacion estandar: {df_train[df_train.columns[1:]].values.std():.2f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=[\"LABEL\"])\n",
    "y = df_train[\"LABEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4775\n",
       "1      33\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 33\n",
      "Before OverSampling, counts of label '0': 4775 \n",
      "\n",
      "After OverSampling, the shape of train_X: (9550, 3197)\n",
      "After OverSampling, the shape of train_y: (9550,) \n",
      "\n",
      "After OverSampling, counts of label '1': 4775\n",
      "After OverSampling, counts of label '0': 4775\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0)))\n",
    "\n",
    "sm = SMOTE(random_state = 22)\n",
    "X_res, y_res = sm.fit_resample(X, y.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_res == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdescaler = StandardScaler()\n",
    "X_scaled = stdescaler.fit_transform(X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_res, test_size=0.24, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3579, 1: 3679}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(*np.unique(y_train, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1196\n",
      "           1       1.00      1.00      1.00      1096\n",
      "\n",
      "    accuracy                           1.00      2292\n",
      "   macro avg       1.00      1.00      1.00      2292\n",
      "weighted avg       1.00      1.00      1.00      2292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Rforest = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "Rforest.fit(X_train, y_train.ravel())\n",
    "predictions = Rforest.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1195    1]\n",
      " [   0 1096]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      1196\n",
      "           1       0.95      0.99      0.97      1096\n",
      "\n",
      "    accuracy                           0.97      2292\n",
      "   macro avg       0.97      0.97      0.97      2292\n",
      "weighted avg       0.97      0.97      0.97      2292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = DecisionTreeClassifier()\n",
    "rfc.fit(X_train, y_train.ravel())\n",
    "predictions = rfc.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1196\n",
      "           1       1.00      1.00      1.00      1096\n",
      "\n",
      "    accuracy                           1.00      2292\n",
      "   macro avg       1.00      1.00      1.00      2292\n",
      "weighted avg       1.00      1.00      1.00      2292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgbCl = xgboost.XGBClassifier()\n",
    "xgbCl.fit(X_train, y_train.ravel())\n",
    "predictions = xgbCl.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1194    2]\n",
      " [   0 1096]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCl.save_model(\"model_xgb.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>FLUX.10</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-171.30</td>\n",
       "      <td>-151.16</td>\n",
       "      <td>-157.96</td>\n",
       "      <td>-167.62</td>\n",
       "      <td>-145.93</td>\n",
       "      <td>-152.78</td>\n",
       "      <td>-153.02</td>\n",
       "      <td>-155.43</td>\n",
       "      <td>-155.21</td>\n",
       "      <td>-152.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.11</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-18.31</td>\n",
       "      <td>-18.73</td>\n",
       "      <td>2.25</td>\n",
       "      <td>6.13</td>\n",
       "      <td>-49.52</td>\n",
       "      <td>-32.39</td>\n",
       "      <td>-22.50</td>\n",
       "      <td>-24.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-504.50</td>\n",
       "      <td>-643.06</td>\n",
       "      <td>-766.56</td>\n",
       "      <td>-899.12</td>\n",
       "      <td>-965.12</td>\n",
       "      <td>-1080.19</td>\n",
       "      <td>-1137.62</td>\n",
       "      <td>-1165.62</td>\n",
       "      <td>-1240.62</td>\n",
       "      <td>-1167.25</td>\n",
       "      <td>...</td>\n",
       "      <td>302.12</td>\n",
       "      <td>351.38</td>\n",
       "      <td>442.25</td>\n",
       "      <td>394.56</td>\n",
       "      <td>545.88</td>\n",
       "      <td>625.00</td>\n",
       "      <td>113.88</td>\n",
       "      <td>81.38</td>\n",
       "      <td>31.69</td>\n",
       "      <td>156.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-224.44</td>\n",
       "      <td>-195.62</td>\n",
       "      <td>-206.94</td>\n",
       "      <td>-329.56</td>\n",
       "      <td>-385.75</td>\n",
       "      <td>-375.94</td>\n",
       "      <td>-455.38</td>\n",
       "      <td>-532.88</td>\n",
       "      <td>-508.69</td>\n",
       "      <td>-586.06</td>\n",
       "      <td>...</td>\n",
       "      <td>132.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>138.88</td>\n",
       "      <td>220.88</td>\n",
       "      <td>136.25</td>\n",
       "      <td>195.62</td>\n",
       "      <td>-145.12</td>\n",
       "      <td>5.44</td>\n",
       "      <td>90.75</td>\n",
       "      <td>121.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159.66</td>\n",
       "      <td>138.18</td>\n",
       "      <td>161.63</td>\n",
       "      <td>149.70</td>\n",
       "      <td>161.82</td>\n",
       "      <td>127.68</td>\n",
       "      <td>157.76</td>\n",
       "      <td>75.66</td>\n",
       "      <td>56.18</td>\n",
       "      <td>77.11</td>\n",
       "      <td>...</td>\n",
       "      <td>64.76</td>\n",
       "      <td>41.37</td>\n",
       "      <td>43.23</td>\n",
       "      <td>1.95</td>\n",
       "      <td>5.24</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>5.31</td>\n",
       "      <td>-7.05</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67.38</td>\n",
       "      <td>69.51</td>\n",
       "      <td>62.63</td>\n",
       "      <td>47.53</td>\n",
       "      <td>48.44</td>\n",
       "      <td>43.56</td>\n",
       "      <td>41.43</td>\n",
       "      <td>43.44</td>\n",
       "      <td>41.44</td>\n",
       "      <td>37.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-3.42</td>\n",
       "      <td>5.12</td>\n",
       "      <td>2.92</td>\n",
       "      <td>4.48</td>\n",
       "      <td>-5.96</td>\n",
       "      <td>16.97</td>\n",
       "      <td>13.36</td>\n",
       "      <td>15.53</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>332.44</td>\n",
       "      <td>287.86</td>\n",
       "      <td>313.24</td>\n",
       "      <td>290.21</td>\n",
       "      <td>271.03</td>\n",
       "      <td>286.99</td>\n",
       "      <td>245.03</td>\n",
       "      <td>248.51</td>\n",
       "      <td>252.49</td>\n",
       "      <td>213.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.41</td>\n",
       "      <td>-16.35</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>-21.26</td>\n",
       "      <td>-83.02</td>\n",
       "      <td>82.24</td>\n",
       "      <td>18.73</td>\n",
       "      <td>29.32</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>-0.42</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>2.25</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>4.88</td>\n",
       "      <td>8.43</td>\n",
       "      <td>9.35</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.95</td>\n",
       "      <td>...</td>\n",
       "      <td>5.42</td>\n",
       "      <td>3.05</td>\n",
       "      <td>7.65</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.72</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>6.65</td>\n",
       "      <td>2.37</td>\n",
       "      <td>4.96</td>\n",
       "      <td>12.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>-131.10</td>\n",
       "      <td>-91.11</td>\n",
       "      <td>-72.02</td>\n",
       "      <td>-85.96</td>\n",
       "      <td>-73.96</td>\n",
       "      <td>-56.25</td>\n",
       "      <td>-48.71</td>\n",
       "      <td>-55.38</td>\n",
       "      <td>-44.19</td>\n",
       "      <td>-62.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.28</td>\n",
       "      <td>8.93</td>\n",
       "      <td>-14.82</td>\n",
       "      <td>41.53</td>\n",
       "      <td>64.06</td>\n",
       "      <td>35.11</td>\n",
       "      <td>-35.44</td>\n",
       "      <td>-38.50</td>\n",
       "      <td>14.67</td>\n",
       "      <td>-1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>323.51</td>\n",
       "      <td>285.75</td>\n",
       "      <td>283.15</td>\n",
       "      <td>237.64</td>\n",
       "      <td>208.31</td>\n",
       "      <td>142.98</td>\n",
       "      <td>78.37</td>\n",
       "      <td>73.03</td>\n",
       "      <td>50.20</td>\n",
       "      <td>23.95</td>\n",
       "      <td>...</td>\n",
       "      <td>-93.96</td>\n",
       "      <td>-99.42</td>\n",
       "      <td>-32.63</td>\n",
       "      <td>-80.77</td>\n",
       "      <td>-29.91</td>\n",
       "      <td>-55.91</td>\n",
       "      <td>-139.32</td>\n",
       "      <td>-130.96</td>\n",
       "      <td>-101.02</td>\n",
       "      <td>-85.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>-11.29</td>\n",
       "      <td>-9.80</td>\n",
       "      <td>-7.80</td>\n",
       "      <td>26.18</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>6.60</td>\n",
       "      <td>5.41</td>\n",
       "      <td>16.48</td>\n",
       "      <td>14.82</td>\n",
       "      <td>15.49</td>\n",
       "      <td>...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>12.01</td>\n",
       "      <td>17.04</td>\n",
       "      <td>14.95</td>\n",
       "      <td>18.06</td>\n",
       "      <td>19.97</td>\n",
       "      <td>-5.75</td>\n",
       "      <td>-7.51</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>-15.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849 rows × 3197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FLUX.1  FLUX.2  FLUX.3  FLUX.4  FLUX.5   FLUX.6   FLUX.7   FLUX.8  \\\n",
       "0   -171.30 -151.16 -157.96 -167.62 -145.93  -152.78  -153.02  -155.43   \n",
       "1   -504.50 -643.06 -766.56 -899.12 -965.12 -1080.19 -1137.62 -1165.62   \n",
       "2   -224.44 -195.62 -206.94 -329.56 -385.75  -375.94  -455.38  -532.88   \n",
       "3    159.66  138.18  161.63  149.70  161.82   127.68   157.76    75.66   \n",
       "4     67.38   69.51   62.63   47.53   48.44    43.56    41.43    43.44   \n",
       "..      ...     ...     ...     ...     ...      ...      ...      ...   \n",
       "844  332.44  287.86  313.24  290.21  271.03   286.99   245.03   248.51   \n",
       "845   -0.42   -1.14    2.25   -3.49    4.88     8.43     9.35     2.80   \n",
       "846 -131.10  -91.11  -72.02  -85.96  -73.96   -56.25   -48.71   -55.38   \n",
       "847  323.51  285.75  283.15  237.64  208.31   142.98    78.37    73.03   \n",
       "848  -11.29   -9.80   -7.80   26.18   -3.48     6.60     5.41    16.48   \n",
       "\n",
       "      FLUX.9  FLUX.10  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0    -155.21  -152.89  ...     -18.11     -17.65     -18.31     -18.73   \n",
       "1   -1240.62 -1167.25  ...     302.12     351.38     442.25     394.56   \n",
       "2    -508.69  -586.06  ...     132.75     134.00     138.88     220.88   \n",
       "3      56.18    77.11  ...      64.76      41.37      43.23       1.95   \n",
       "4      41.44    37.09  ...       1.97      -3.42       5.12       2.92   \n",
       "..       ...      ...  ...        ...        ...        ...        ...   \n",
       "844   252.49   213.02  ...     -16.41     -16.35       1.80      -3.46   \n",
       "845     1.90     2.95  ...       5.42       3.05       7.65      11.01   \n",
       "846   -44.19   -62.16  ...     -37.28       8.93     -14.82      41.53   \n",
       "847    50.20    23.95  ...     -93.96     -99.42     -32.63     -80.77   \n",
       "848    14.82    15.49  ...       4.80      12.01      17.04      14.95   \n",
       "\n",
       "     FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0         2.25       6.13     -49.52     -32.39     -22.50     -24.50  \n",
       "1       545.88     625.00     113.88      81.38      31.69     156.06  \n",
       "2       136.25     195.62    -145.12       5.44      90.75     121.81  \n",
       "3         5.24      -2.81       5.31      -7.05       2.67      18.25  \n",
       "4         4.48      -5.96      16.97      13.36      15.53       7.70  \n",
       "..         ...        ...        ...        ...        ...        ...  \n",
       "844     -21.26     -83.02      82.24      18.73      29.32      11.98  \n",
       "845      10.72      -4.03       6.65       2.37       4.96      12.13  \n",
       "846      64.06      35.11     -35.44     -38.50      14.67      -1.21  \n",
       "847     -29.91     -55.91    -139.32    -130.96    -101.02     -85.14  \n",
       "848      18.06      19.97      -5.75      -7.51      -9.21     -15.96  \n",
       "\n",
       "[849 rows x 3197 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = stdescaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones_datathon_rforest = Rforest.predict(X_test_scaled)\n",
    "predicciones_datathon_rforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 849}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(*np.unique(predicciones_datathon_rforest, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones_datathon_xgb = xgbCl.predict(X_test_scaled)\n",
    "predicciones_datathon_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 847, 1: 2}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(*np.unique(predicciones_datathon_xgb, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LABEL\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "..     ...\n",
       "844      0\n",
       "845      0\n",
       "846      0\n",
       "847      0\n",
       "848      0\n",
       "\n",
       "[849 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado = pd.DataFrame(predicciones_datathon_xgb, columns=[\"LABEL\"], dtype=\"int16\")\n",
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LABEL\n",
       "299      1\n",
       "460      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado[df_resultado[\"LABEL\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado.to_csv(\"prediccion.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "xgbClassifier = xgboost.XGBClassifier()\n",
    "classifiers.append(xgbClassifier)\n",
    "\n",
    "svc = svm.SVC()\n",
    "classifiers.append(svc)\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "classifiers.append(dtc)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "classifiers.append(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...) is 0.9991273996509599\n",
      "\n",
      "Confusion Matrix of XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...) is [[1194    2]\n",
      " [   0 1096]]\n",
      "\n",
      "\n",
      "Accuracy of SVC() is 0.5680628272251309\n",
      "\n",
      "Confusion Matrix of SVC() is [[1187    9]\n",
      " [ 981  115]]\n",
      "\n",
      "\n",
      "Accuracy of DecisionTreeClassifier() is 0.9777486910994765\n",
      "\n",
      "Confusion Matrix of DecisionTreeClassifier() is [[1155   41]\n",
      " [  10 1086]]\n",
      "\n",
      "\n",
      "Accuracy of RandomForestClassifier() is 0.9995636998254799\n",
      "\n",
      "Confusion Matrix of RandomForestClassifier() is [[1195    1]\n",
      " [   0 1096]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred= clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy of %s is %s\\n\" % (clf, acc))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix of %s is %s\\n\\n\" % (clf, cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6611db47884cdc8ff80a23c17f01b7c6a32184fb35d608b02f6349b31cf7494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
