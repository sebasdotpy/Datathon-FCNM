{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` py\n",
    "The contents of this script are\n",
    "#  1. Importing Libraries\n",
    "#  2. Get data\n",
    "#  3. Create train and test set\n",
    "#  4. Classifiers\n",
    "#  5. Hyper-parameters\n",
    "#  6. Feature Selection: Removing highly correlated features\n",
    "#  7. Tuning a classifier to use with RFECV\n",
    "#  8. Custom pipeline object to use with RFECV\n",
    "#  9. Feature Selection: Recursive Feature Selection with Cross Validation\n",
    "#  10. Performance Curve\n",
    "#  11. Feature Selection: Recursive Feature Selection\n",
    "#  12. Visualizing Selected Features Importance\n",
    "#  13. Classifier Tuning and Evaluation\n",
    "#  14. Visualing Results\n",
    "\n",
    "# * --------------------------------------------------------------------------\n",
    "# * AUTHORS(S): Frank Ceballos <frank.ceballos89@gmail.com>\n",
    "# * --------------------------------------------------------------------------\n",
    "# * DATE CREATED: June 26, 2019\n",
    "# * --------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                          1. Importing Libraries                             #\n",
    "###############################################################################\n",
    "# For reading, visualizing, and preprocessing data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                 2. Get data                                 #\n",
    "###############################################################################\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['LABEL'] = df_train['LABEL'].apply(lambda x: 0 if x==1 else 1)\n",
    "\n",
    "# Numpy array to pandas dataframe\n",
    "X = df_train.drop(columns=[\"LABEL\"])\n",
    "y = df_train[\"LABEL\"]\n",
    "\n",
    "# Solve the problem of the unbalanced data\n",
    "sm = SMOTE(random_state = 42)\n",
    "X, y = sm.fit_resample(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                        3. Create train and test set                         #\n",
    "###############################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                               4. Classifiers                                #\n",
    "###############################################################################\n",
    "# Create list of tuples with classifier label and classifier object\n",
    "classifiers = {}\n",
    "classifiers.update({\"LDA\": LinearDiscriminantAnalysis()})\n",
    "classifiers.update({\"QDA\": QuadraticDiscriminantAnalysis()})\n",
    "classifiers.update({\"AdaBoost\": AdaBoostClassifier()})\n",
    "classifiers.update({\"Bagging\": BaggingClassifier()})\n",
    "classifiers.update({\"Extra Trees Ensemble\": ExtraTreesClassifier()})\n",
    "classifiers.update({\"Gradient Boosting\": GradientBoostingClassifier()})\n",
    "classifiers.update({\"Random Forest\": RandomForestClassifier()})\n",
    "classifiers.update({\"Ridge\": RidgeClassifier()})\n",
    "classifiers.update({\"SGD\": SGDClassifier()})\n",
    "classifiers.update({\"BNB\": BernoulliNB()})\n",
    "classifiers.update({\"GNB\": GaussianNB()})\n",
    "classifiers.update({\"KNN\": KNeighborsClassifier()})\n",
    "classifiers.update({\"MLP\": MLPClassifier()})\n",
    "classifiers.update({\"LSVC\": LinearSVC()})\n",
    "classifiers.update({\"NuSVC\": NuSVC()})\n",
    "classifiers.update({\"SVC\": SVC()})\n",
    "classifiers.update({\"DTC\": DecisionTreeClassifier()})\n",
    "classifiers.update({\"ETC\": ExtraTreeClassifier()})\n",
    "classifiers.update({\"XGBClassifier\": XGBClassifier()})\n",
    "\n",
    "# Create dict of decision function labels\n",
    "DECISION_FUNCTIONS = {\"Ridge\", \"SGD\", \"LSVC\", \"NuSVC\", \"SVC\"}\n",
    "\n",
    "# Create dict for classifiers with feature_importances_ attribute\n",
    "FEATURE_IMPORTANCE = {\"Gradient Boosting\", \"Extra Trees Ensemble\", \"Random Forest\", \"XGBClassifier\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                             5. Hyper-parameters                             #\n",
    "###############################################################################\n",
    "# Initiate parameter grid\n",
    "parameters = {}\n",
    "\n",
    "# Update dict with LDA\n",
    "parameters.update({\"LDA\": {\"classifier__solver\": [\"svd\"]}})\n",
    "\n",
    "# Update dict with QDA\n",
    "parameters.update({\"QDA\": {\"classifier__reg_param\":[0.01*ii for ii in range(0, 101)]}})\n",
    "# Update dict with AdaBoost\n",
    "parameters.update({\"AdaBoost\": {\n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__learning_rate\": [0.001, 0.01, 0.05, 0.1, 0.25, 0.50, 0.75, 1.0]\n",
    "                                }})\n",
    "\n",
    "# Update dict with Bagging\n",
    "parameters.update({\"Bagging\": {\n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__max_features\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                \"classifier__n_jobs\": [-1]\n",
    "                                }})\n",
    "\n",
    "# Update dict with Gradient Boosting\n",
    "parameters.update({\"Gradient Boosting\": {\n",
    "                                        \"classifier__learning_rate\":[0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "                                        \"classifier__n_estimators\": [200],\n",
    "                                        \"classifier__max_depth\": [2,3,4,5,6],\n",
    "                                        \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                        \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                        \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                        \"classifier__subsample\": [0.8, 0.9, 1]\n",
    "                                        }})\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "parameters.update({\"Extra Trees Ensemble\": {\n",
    "                                            \"classifier__n_estimators\": [200],\n",
    "                                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                            \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                            \"classifier__n_jobs\": [-1]\n",
    "                                            }})\n",
    "\n",
    "\n",
    "# Update dict with Random Forest Parameters\n",
    "parameters.update({\"Random Forest\": {\n",
    "                                    \"classifier__n_estimators\": [200],\n",
    "                                    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                    \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                    \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                    \"classifier__n_jobs\": [-1]\n",
    "                                    }})\n",
    "\n",
    "# Update dict with Ridge\n",
    "parameters.update({\"Ridge\": {\n",
    "                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0]\n",
    "                            }})\n",
    "\n",
    "# Update dict with SGD Classifier\n",
    "parameters.update({\"SGD\": {\n",
    "                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0],\n",
    "                            \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                            \"classifier__n_jobs\": [-1]\n",
    "                            }})\n",
    "\n",
    "\n",
    "# Update dict with BernoulliNB Classifier\n",
    "parameters.update({\"BNB\": {\n",
    "                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0]\n",
    "                            }})\n",
    "\n",
    "# Update dict with GaussianNB Classifier\n",
    "parameters.update({\"GNB\": {\n",
    "                            \"classifier__var_smoothing\": [1e-9, 1e-8,1e-7, 1e-6, 1e-5]\n",
    "                            }})\n",
    "\n",
    "# Update dict with K Nearest Neighbors Classifier\n",
    "parameters.update({\"KNN\": {\n",
    "                            \"classifier__n_neighbors\": list(range(1,31)),\n",
    "                            \"classifier__p\": [1, 2, 3, 4, 5],\n",
    "                            \"classifier__leaf_size\": [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "                            \"classifier__n_jobs\": [-1]\n",
    "                            }})\n",
    "\n",
    "# Update dict with MLPClassifier\n",
    "parameters.update({\"MLP\": {\n",
    "                            \"classifier__hidden_layer_sizes\": [(5), (10), (5,5), (10,10), (5,5,5), (10,10,10)],\n",
    "                            \"classifier__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                            \"classifier__learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "                            \"classifier__max_iter\": [100, 200, 300, 500, 1000, 2000],\n",
    "                            \"classifier__alpha\": list(10.0 ** -np.arange(1, 10)),\n",
    "                            }})\n",
    "\n",
    "parameters.update({\"LSVC\": {\n",
    "                            \"classifier__penalty\": [\"l2\"],\n",
    "                            \"classifier__C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "                            }})\n",
    "\n",
    "parameters.update({\"NuSVC\": {\n",
    "                            \"classifier__nu\": [0.25, 0.50, 0.75],\n",
    "                            \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                            \"classifier__degree\": [1,2,3,4,5,6],\n",
    "                            }})\n",
    "\n",
    "parameters.update({\"SVC\": {\n",
    "                            \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                            \"classifier__gamma\": [\"auto\"],\n",
    "                            \"classifier__C\": [0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "                            \"classifier__degree\": [1, 2, 3, 4, 5, 6]\n",
    "                            }})\n",
    "\n",
    "\n",
    "# Update dict with Decision Tree Classifier\n",
    "parameters.update({\"DTC\": {\n",
    "                            \"classifier__criterion\" :[\"gini\", \"entropy\"],\n",
    "                            \"classifier__splitter\": [\"best\", \"random\"],\n",
    "                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                            \"classifier__max_depth\" : [1,2,3, 4, 5, 6, 7, 8],\n",
    "                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            }})\n",
    "\n",
    "# Update dict with Extra Tree Classifier\n",
    "parameters.update({\"ETC\": {\n",
    "                            \"classifier__criterion\" :[\"gini\", \"entropy\"],\n",
    "                            \"classifier__splitter\": [\"best\", \"random\"],\n",
    "                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                            \"classifier__max_depth\" : [1,2,3, 4, 5, 6, 7, 8],\n",
    "                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            }})\n",
    "\n",
    "# Update dict with Random Forest Parameters\n",
    "parameters.update({\"XGBClassifier\": {\n",
    "                            # 'max_depth': [1,2,3,4,5,8,10,12],\n",
    "                            # 'gamma': [1,2,4,6,9],\n",
    "                            # 'reg_alpha' : [40,50,75,100,140,180],\n",
    "                            # 'reg_lambda' : np.arange(0.01,1,0.05),\n",
    "                            # 'colsample_bytree' : np.arange(0.4,1,0.1),\n",
    "                            # 'min_child_weight' : np.arange(0, 10),\n",
    "                            # 'n_estimators': [100, 160, 200],\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#              6. Feature Selection: Removing highly correlated features      #\n",
    "###############################################################################\n",
    "# Filter Method: Spearman's Cross Correlation > 0.95\n",
    "# Make correlation matrix\n",
    "corr_matrix = X_train.corr(method = \"spearman\").abs()\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.set(font_scale = 1.0)\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr_matrix, cmap= \"YlGnBu\", square=True, ax = ax)\n",
    "f.tight_layout()\n",
    "plt.savefig(\"correlation_matrix.png\", dpi = 1080)\n",
    "\n",
    "# Select upper triangle of matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "X_train = X_train.drop(to_drop, axis = 1)\n",
    "X_test = X_test.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                Base Estimators                              #\n",
    "###############################################################################\n",
    "# Create dict for classifiers with feature_importances_ attribute\n",
    "FEATURE_IMPORTANCE = {\"Gradient Boosting\", \"Extra Trees Ensemble\", \"Random Forest\", \"XGBClassifier\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tuning XGBClassifier. Go grab a beer or something.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#                     7. Tuning a classifier to use with RFECV                #\n",
    "###############################################################################\n",
    "# Define classifier to use as the base of the recursive feature elimination algorithm\n",
    "selected_classifier = \"XGBClassifier\"\n",
    "classifier = classifiers[selected_classifier]\n",
    "\n",
    "# Tune classifier (Took = 4.8 minutes)\n",
    "\n",
    "# Scale features via Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define steps in pipeline\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "\n",
    "# Initialize Pipeline object\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = parameters[selected_classifier]\n",
    "\n",
    "# Initialize GridSearch object\n",
    "gscv = GridSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = \"recall\")\n",
    "\n",
    "# Fit gscv\n",
    "print(f\"Now tuning {selected_classifier}. Go grab a beer or something.\")\n",
    "gscv.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = gscv.best_params_\n",
    "best_score = gscv.best_score_\n",
    "\n",
    "# Update classifier parameters\n",
    "tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "classifier.set_params(**tuned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  8. Custom pipeline object to use with RFECV                #\n",
    "###############################################################################\n",
    "# Select Features using RFECV\n",
    "class PipelineRFE(Pipeline):\n",
    "    # Source: https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        super(PipelineRFE, self).fit(X, y, **fit_params)\n",
    "        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#   9. Feature Selection: Recursive Feature Selection with Cross Validation   #\n",
    "###############################################################################\n",
    "# Define pipeline for RFECV\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFECV object\n",
    "feature_selector = RFECV(pipe, cv = 5, step = 1, scoring = \"roc_auc\", verbose = 1)\n",
    "\n",
    "# Fit RFECV\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                             10. Performance Curve                           #\n",
    "###############################################################################\n",
    "# Get Performance Data\n",
    "performance_curve = {\"Number of Features\": list(range(1, len(feature_names) + 1)),\n",
    "                    \"AUC\": feature_selector.grid_scores_}\n",
    "performance_curve = pd.DataFrame(performance_curve)\n",
    "\n",
    "# Performance vs Number of Features\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "colors = sns.color_palette(\"RdYlGn\", 20)\n",
    "line_color = colors[3]\n",
    "marker_colors = colors[-1]\n",
    "\n",
    "# Plot\n",
    "f, ax = plt.subplots(figsize=(13, 6.5))\n",
    "sns.lineplot(x = \"Number of Features\", y = \"AUC\", data = performance_curve,\n",
    "             color = line_color, lw = 4, ax = ax)\n",
    "sns.regplot(x = performance_curve[\"Number of Features\"], y = performance_curve[\"AUC\"],\n",
    "            color = marker_colors, fit_reg = False, scatter_kws = {\"s\": 200}, ax = ax)\n",
    "\n",
    "# Axes limits\n",
    "plt.xlim(0.5, len(feature_names)+0.5)\n",
    "plt.ylim(0.60, 0.925)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axhline(y = 0.625, color = 'black', linewidth = 1.3, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure\n",
    "plt.savefig(\"performance_curve.png\", dpi = 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                11. Feature Selection: Recursive Feature Selection           #\n",
    "###############################################################################\n",
    "# Define pipeline for RFECV\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFE object\n",
    "feature_selector = RFE(pipe, n_features_to_select = 10, step = 1, verbose = 1)\n",
    "\n",
    "# Fit RFE\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features labels\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                  12. Visualizing Selected Features Importance               #\n",
    "###############################################################################\n",
    "# Get selected features data set\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Train classifier\n",
    "classifier.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame(selected_features, columns = [\"Feature Label\"])\n",
    "feature_importance[\"Feature Importance\"] = classifier.feature_importances_\n",
    "\n",
    "# Sort by feature importance\n",
    "feature_importance = feature_importance.sort_values(by=\"Feature Importance\", ascending=False)\n",
    "\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "\n",
    "# Set figure size and create barplot\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.barplot(x = \"Feature Importance\", y = \"Feature Label\",\n",
    "            palette = reversed(sns.color_palette('YlOrRd', 15)),  data = feature_importance)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure\n",
    "plt.savefig(\"feature_importance.png\", dpi = 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                       13. Classifier Tuning and Evaluation                  #\n",
    "###############################################################################\n",
    "# Initialize dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Tune and evaluate classifiers\n",
    "for classifier_label, classifier in classifiers.items():\n",
    "    # Print message to user\n",
    "    print(f\"Now tuning {classifier_label}.\")\n",
    "\n",
    "    # Scale features via Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define steps in pipeline\n",
    "    steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = parameters[classifier_label]\n",
    "\n",
    "    # Initialize GridSearch object\n",
    "    gscv = GridSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n",
    "\n",
    "    # Fit gscv\n",
    "    gscv.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "    # Get best parameters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "\n",
    "    # Update classifier parameters and define new pipeline with tuned classifier\n",
    "    tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "    classifier.set_params(**tuned_params)\n",
    "\n",
    "    # Make predictions\n",
    "    if classifier_label in DECISION_FUNCTIONS:\n",
    "        y_pred = gscv.decision_function(X_test)\n",
    "    else:\n",
    "        y_pred = gscv.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Evaluate model\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Save results\n",
    "    result = {\"Classifier\": gscv,\n",
    "              \"Best Parameters\": best_params,\n",
    "              \"Training AUC\": best_score,\n",
    "              \"Test AUC\": auc}\n",
    "\n",
    "    results.update({classifier_label: result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                              14. Visualing Results                          #\n",
    "###############################################################################\n",
    "# Initialize auc_score dictionary\n",
    "auc_scores = {\n",
    "            \"Classifier\": [],\n",
    "            \"AUC\": [],\n",
    "            \"AUC Type\": []\n",
    "            }\n",
    "\n",
    "# Get AUC scores into dictionary\n",
    "for classifier_label in results:\n",
    "    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n",
    "                       \"AUC\": [results[classifier_label][\"Training AUC\"]] + auc_scores[\"AUC\"],\n",
    "                       \"AUC Type\": [\"Training\"] + auc_scores[\"AUC Type\"]})\n",
    "\n",
    "    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n",
    "                       \"AUC\": [results[classifier_label][\"Test AUC\"]] + auc_scores[\"AUC\"],\n",
    "                       \"AUC Type\": [\"Test\"] + auc_scores[\"AUC Type\"]})\n",
    "\n",
    "# Dictionary to PandasDataFrame\n",
    "auc_scores = pd.DataFrame(auc_scores)\n",
    "\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "\n",
    "\n",
    "# Colors\n",
    "training_color = sns.color_palette(\"RdYlBu\", 10)[1]\n",
    "test_color = sns.color_palette(\"RdYlBu\", 10)[-2]\n",
    "colors = [training_color, test_color]\n",
    "\n",
    "# Set figure size and create barplot\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "sns.barplot(x=\"AUC\", y=\"Classifier\", hue=\"AUC Type\", palette = colors,\n",
    "            data=auc_scores)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure\n",
    "plt.savefig(\"AUC Scores.png\", dpi = 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6611db47884cdc8ff80a23c17f01b7c6a32184fb35d608b02f6349b31cf7494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
